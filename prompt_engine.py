# app.py
# prompt_engine.py
"""
IMD Sales Bot - Main Application
ë‹¤í¬ ì—˜ë ˆê°•ìŠ¤ (McKinsey ì»¨ì„¤íŒ… ìŠ¤íƒ€ì¼)
IMD Sales Bot - AI Response Generation
Gemini ì „ìš© + Fallback ë°©ì§€ ê°•í™”
"""

import streamlit as st
import google.generativeai as genai
from typing import Dict, Optional
import time
from conversation_manager import get_conversation_manager
from prompt_engine import get_prompt_engine, generate_ai_response
from lead_handler import LeadHandler
from config import (
    APP_TITLE,
    APP_ICON,
    LAYOUT,
    COLOR_PRIMARY,
    COLOR_SECONDARY,
    COLOR_BG,
    COLOR_TEXT,
    COLOR_AI_BUBBLE,
    COLOR_USER_BUBBLE,
    COLOR_BORDER,
    URGENCY_OPTIONS
    SYSTEM_PROMPT,
    GEMINI_MODEL,
    GEMINI_TEMPERATURE,
    GEMINI_MAX_TOKENS,
    CASE_STUDIES,
    MAX_RETRY_ATTEMPTS
)

# ============================================
# 0. í˜ì´ì§€ ì„¤ì •
# ============================================
st.set_page_config(
    page_title=APP_TITLE,
    page_icon=APP_ICON,
    layout=LAYOUT
)

# ============================================
# 1. CSS ìŠ¤íƒ€ì¼ë§ (ë‹¤í¬ ì—˜ë ˆê°•ìŠ¤)
# ============================================
def load_css():
    """ë‹¤í¬ ì—˜ë ˆê°•ìŠ¤ CSS"""
    custom_css = f"""
    <style>
    /* ì „ì²´ ë°°ê²½ */
    .stApp {{
        background: linear-gradient(135deg, {COLOR_BG} 0%, #1a1f35 100%);
        font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, sans-serif;
        color: {COLOR_TEXT};
    }}
    
    /* íƒ€ì´í‹€ */
    h1 {{
        color: {COLOR_PRIMARY} !important;
        font-weight: 700;
        text-align: center;
        letter-spacing: -0.5px;
        margin-bottom: 8px;
    }}
    
    h2, h3 {{
        color: {COLOR_TEXT} !important;
        font-weight: 600;
    }}
    
    /* ì„œë¸Œíƒ€ì´í‹€ */
    .subtitle {{
        text-align: center;
        color: #94A3B8;
        font-size: 15px;
        margin-bottom: 32px;
        font-weight: 400;
    }}
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ */
    .chat-container {{
        max-width: 720px;
        margin: 24px auto;
        padding-bottom: 100px;
    }}
    
    /* AI ë©”ì‹œì§€ ë²„ë¸” */
    .chat-bubble-ai {{
        background: linear-gradient(135deg, {COLOR_AI_BUBBLE} 0%, #2d3748 100%);
        color: {COLOR_TEXT} !important;
        padding: 20px 24px;
        border-radius: 16px 16px 16px 4px;
        margin-bottom: 16px;
        width: fit-content;
        max-width: 85%;
        font-size: 15px;
        line-height: 1.7;
        border-left: 3px solid {COLOR_PRIMARY};
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
        animation: fadeIn 0.6s ease;
    }}
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ ë²„ë¸” */
    .chat-bubble-user {{
        background: {COLOR_USER_BUBBLE};
        color: {COLOR_TEXT} !important;
        padding: 16px 24px;
        border-radius: 16px 16px 4px 16px;
        margin-bottom: 16px;
        margin-left: auto;
        width: fit-content;
        max-width: 75%;
        font-size: 15px;
        font-weight: 500;
        animation: slideIn 0.4s ease;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
        border: 1px solid {COLOR_BORDER};
    }}
    
    /* ì• ë‹ˆë©”ì´ì…˜ */
    @keyframes fadeIn {{
        from {{ opacity: 0; transform: translateY(12px); }}
        to {{ opacity: 1; transform: translateY(0); }}
    }}
class PromptEngine:
    """AI ì‘ë‹µ ìƒì„± ì—”ì§„ (Fallback ë°©ì§€ ê°•í™”)"""

    @keyframes slideIn {{
        from {{ opacity: 0; transform: translateX(12px); }}
        to {{ opacity: 1; transform: translateX(0); }}
    }}
    def __init__(self):
        """Gemini API ì´ˆê¸°í™”"""
        self.model = None
        self.retry_count = 0
        self._init_gemini()

    /* ì¶”ì²œ ë²„íŠ¼ */
    .stButton > button {{
        width: 100%;
        background: transparent;
        color: {COLOR_PRIMARY} !important;
        border: 1.5px solid {COLOR_BORDER};
        padding: 14px 20px;
        font-size: 14px;
        border-radius: 12px;
        font-weight: 500;
        transition: all 0.3s ease;
        margin-bottom: 8px;
        letter-spacing: 0.3px;
    }}
    
    .stButton > button:hover {{
        background: {COLOR_AI_BUBBLE};
        border-color: {COLOR_PRIMARY};
        box-shadow: 0 0 16px rgba(212, 175, 55, 0.2);
        transform: translateY(-2px);
    }}
    
    /* ì…ë ¥ì°½ */
    .stChatInput > div {{
        background-color: {COLOR_AI_BUBBLE} !important;
        border: 1px solid {COLOR_BORDER} !important;
        border-radius: 12px !important;
    }}
    
    input[type="text"], textarea, .stSelectbox > div > div {{
        background-color: {COLOR_AI_BUBBLE} !important;
        color: {COLOR_TEXT} !important;
        border: 1px solid {COLOR_BORDER} !important;
        border-radius: 8px !important;
        padding: 12px !important;
    }}
    
    /* í¼ ìŠ¤íƒ€ì¼ */
    .stForm {{
        background: linear-gradient(135deg, {COLOR_AI_BUBBLE} 0%, #2d3748 100%);
        padding: 28px;
        border-radius: 16px;
        border: 1px solid {COLOR_PRIMARY};
        box-shadow: 0 8px 24px rgba(0, 0, 0, 0.4);
    }}
    def _init_gemini(self):
        """Gemini API ì„¤ì •"""
        try:
            if "GEMINI_API_KEY" not in st.secrets:
                st.error("âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                self.model = None
                return
            
            api_key = st.secrets["GEMINI_API_KEY"]
            
            if not api_key:
                st.error("âŒ API í‚¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                self.model = None
                return
            
            # Gemini ì„¤ì •
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(
                model_name=GEMINI_MODEL,
                generation_config={
                    "temperature": GEMINI_TEMPERATURE,
                    "max_output_tokens": GEMINI_MAX_TOKENS,
                }
            )
            
            # ì´ˆê¸°í™” ì„±ê³µ í‘œì‹œ (í•œë²ˆë§Œ)
            if 'gemini_initialized' not in st.session_state:
                st.success(f"âœ… AI ì»¨ì„¤í„´íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
                st.session_state.gemini_initialized = True
            
        except Exception as e:
            st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.model = None

    /* ì„¹ì…˜ ì œëª© */
    .section-title {{
        color: {COLOR_PRIMARY};
        font-size: 18px;
        font-weight: 600;
        margin: 24px 0 12px 0;
        text-align: center;
    }}
    def generate_response(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """
        AI ì‘ë‹µ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        """
        if not self.model:
            st.error("âš ï¸ AI ì‹œìŠ¤í…œì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
            return self._fallback_response(user_input, context)
        
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(MAX_RETRY_ATTEMPTS):
            try:
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                full_prompt = self._build_prompt(user_input, context, conversation_history)
                
                # Gemini í˜¸ì¶œ
                response = self.model.generate_content(full_prompt)
                
                # ì‘ë‹µ ê²€ì¦
                if not response or not response.text:
                    raise ValueError("ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                
                # ì‘ë‹µ í›„ì²˜ë¦¬
                ai_response = self._post_process_response(response.text.strip(), context)
                
                # ì„±ê³µ ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                self.retry_count = 0
                
                return ai_response
                
            except Exception as e:
                self.retry_count += 1
                error_msg = str(e)
                
                # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œ ì‹¤íŒ¨
                if attempt == MAX_RETRY_ATTEMPTS - 1:
                    st.error(f"âš ï¸ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRY_ATTEMPTS})")
                    st.error(f"ì˜¤ë¥˜: {error_msg}")
                    
                    # ì—ëŸ¬ íƒ€ì…ë³„ ì•ˆë‚´
                    if "quota" in error_msg.lower() or "rate" in error_msg.lower():
                        st.warning("ğŸ’¡ API ì‚¬ìš©ëŸ‰ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    elif "invalid" in error_msg.lower():
                        st.warning("ğŸ’¡ API í‚¤ ì˜¤ë¥˜. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
                    
                    return self._fallback_response(user_input, context)
                else:
                    # ì¬ì‹œë„
                    st.warning(f"ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{MAX_RETRY_ATTEMPTS})")
                    time.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        
        return self._fallback_response(user_input, context)

    /* êµ¬ë¶„ì„  */
    hr {{
        border-color: {COLOR_BORDER};
        opacity: 0.3;
    }}
    </style>
    """
    st.markdown(custom_css, unsafe_allow_html=True)

load_css()

# ============================================
# 2. ì´ˆê¸°í™”
# ============================================
conv_manager = get_conversation_manager()
prompt_engine = get_prompt_engine()
lead_handler = LeadHandler()
    def _build_prompt(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¡°ë¦½"""
        system_prompt = SYSTEM_PROMPT.format(
            user_type=context.get('user_type') or 'ë¯¸íŒŒì•…',
            pain_point=context.get('pain_point') or 'ë¯¸íŒŒì•…',
            stage=context.get('stage', 'initial'),
            trust_level=context.get('trust_level', 0)
        )
        
        # ë°˜ë°• ì‚¬í•­ ëŒ€ì‘
        if context.get('objections'):
            objection_guide = self._get_objection_handling(context['objections'])
            system_prompt += f"\n\n## í˜„ì¬ ê³ ê° ìš°ë ¤ì‚¬í•­\n{objection_guide}"
        
        # ì‚¬ë¡€ ì—°êµ¬ ì¶”ê°€
        if context.get('user_type') in CASE_STUDIES:
            case = CASE_STUDIES[context['user_type']]
            system_prompt += f"\n\n## ì œì‹œ ê°€ëŠ¥í•œ ì‹¤ì œ ì‚¬ë¡€\n- {case['title']}: {case['result']}\n- ì •ëŸ‰ ë°ì´í„°: {case['data']}"
        
        full_prompt = f"""{system_prompt}

# ì²« ë°©ë¬¸ ì‹œ ì›°ì»´ ë©”ì‹œì§€
if len(conv_manager.get_history()) == 0:
    initial_msg = prompt_engine.generate_initial_message()
    conv_manager.add_message("ai", initial_msg)
---

# ============================================
# 3. í—¤ë”
# ============================================
st.title("IMD AI ì „ëµ ì»¨ì„¤íŒ…")
st.markdown('<p class="subtitle">ë°ì´í„° ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ì¥ ì†”ë£¨ì…˜</p>', unsafe_allow_html=True)
## ìµœê·¼ ëŒ€í™” ë‚´ì—­
{conversation_history}

# ============================================
# 4. ì±„íŒ… íˆìŠ¤í† ë¦¬ ë Œë”ë§
# ============================================
st.markdown('<div class="chat-container">', unsafe_allow_html=True)
---

for chat in conv_manager.get_history():
    role_class = "chat-bubble-ai" if chat['role'] == 'ai' else "chat-bubble-user"
    st.markdown(f'<div class="{role_class}">{chat["text"]}</div>', unsafe_allow_html=True)
## ê³ ê°ì˜ ìµœì‹  ì…ë ¥
{user_input}

st.markdown('</div>', unsafe_allow_html=True)
---

# ============================================
# 5. ì¶”ì²œ ë²„íŠ¼
# ============================================
if not conv_manager.is_ready_for_conversion():
    st.markdown('<p class="section-title">ì£¼ìš” ë¬¸ì˜ ì‚¬í•­</p>', unsafe_allow_html=True)
**ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ì‘ë‹µí•˜ì„¸ìš”. 2-4ë¬¸ì¥, ë°ì´í„° ê¸°ë°˜, ëª…í™•í•œ ì œì•ˆ.**
"""
        return full_prompt

    buttons = conv_manager.get_recommended_buttons()
    def _get_objection_handling(self, objections: list) -> str:
        """ë°˜ë°• ì‚¬í•­ ëŒ€ì‘ ì „ëµ"""
        strategies = {
            'skeptical': "â†’ ì •ëŸ‰ ë°ì´í„°ë¡œ ì¦ëª…. 'ë„ì… 6ê°œì›”, ì „í™˜ìœ¨ 32% ìƒìŠ¹' ê°™ì€ êµ¬ì²´ì  ìˆ˜ì¹˜ ì œì‹œ",
            'complexity': "â†’ 'ì´ˆê¸° ì…‹ì—… 3ì¼, ì§ì› êµìœ¡ 2ì‹œê°„' ê°™ì´ êµ¬ì²´ì  ì¼ì • ëª…ì‹œ",
            'price_sensitive': "â†’ ROI ì¤‘ì‹¬. 'ì›” 200ë§Œì› íˆ¬ì ì‹œ íšŒìˆ˜ ê¸°ê°„ 3ê°œì›”, ì—°ê°„ 2,400ë§Œì› ì¶”ê°€ ë§¤ì¶œ'"
        }
        
        guide = [strategies[obj] for obj in objections if obj in strategies]
        return "\n".join(guide) if guide else "ë°ì´í„° ê¸°ë°˜ í•´ê²°ì±… ì œì‹œ"

    # ë²„íŠ¼ ë ˆì´ì•„ì›ƒ
    if len(buttons) == 3:
        cols = st.columns(3)
    else:
        cols = st.columns(len(buttons))
    def _post_process_response(self, response: str, context: Dict) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        import re
        
        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        response = response.replace('\n\n\n', '\n\n')
        
        # ë§ˆí¬ë‹¤ìš´ êµµê¸° ì²˜ë¦¬
        response = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', response)
        
        # ê¸¸ì´ ì œí•œ (600ì)
        if len(response) > 600:
            response = response[:580] + "\n\nì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?"
        
        # ê¸ˆì§€ ë‹¨ì–´ í•„í„°
        forbidden_words = {
            'LLM': 'AI ì‹œìŠ¤í…œ',
            'RAG': 'AI ê¸°ìˆ ',
            'API': 'ì‹œìŠ¤í…œ',
            'ë¨¸ì‹ ëŸ¬ë‹': 'AI',
            'ë”¥ëŸ¬ë‹': 'AI'
        }
        
        for word, replacement in forbidden_words.items():
            response = response.replace(word, replacement)
        
        # CTA ì¶”ê°€ (ì ì ˆí•œ íƒ€ì´ë°)
        if context.get('trust_level', 0) >= 50 and 'ì„¤ê³„' not in response.lower():
            response += "\n\nì•„ë˜ ì •ë³´ë¥¼ ë‚¨ê²¨ì£¼ì‹œë©´ ë§ì¶¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë°œì†¡í•´ë“œë¦½ë‹ˆë‹¤."
        
        return response

    for idx, button_text in enumerate(buttons):
        with cols[idx]:
            if st.button(button_text, key=f"quick_{idx}"):
                # ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
                conv_manager.add_message("user", button_text, metadata={"type": "button"})
                
                # AI ì‘ë‹µ ìƒì„±
                context = conv_manager.get_context()
                history = conv_manager.get_formatted_history(for_llm=True)
                
                with st.spinner("ë¶„ì„ ì¤‘..."):
                    time.sleep(0.8)
                    ai_response = generate_ai_response(button_text, context, history)
                
                conv_manager.add_message("ai", ai_response)
                st.rerun()
    def _fallback_response(self, user_input: str, context: Dict) -> str:
        """Fallback ì‘ë‹µ (ìµœì†Œí•œìœ¼ë¡œë§Œ ì‚¬ìš©)"""
        user_lower = user_input.lower()
        
        # ê°€ê²© ë¬¸ì˜
        if any(word in user_lower for word in ['ê°€ê²©', 'ë¹„ìš©', 'ì–¼ë§ˆ', 'ìš”ê¸ˆ']):
            return """íˆ¬ì ê¸ˆì•¡ë³´ë‹¤ ì¤‘ìš”í•œ ê²ƒì€ íˆ¬ì íšŒìˆ˜ ê¸°ê°„ì…ë‹ˆë‹¤.

# ============================================
# 6. ì±„íŒ… ì…ë ¥ì°½
# ============================================
user_input = st.chat_input("ë¬¸ì˜ ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”")
í˜„ì¬ ì›” ë°©ë¬¸ì ìˆ˜ì™€ ì „í™˜ìœ¨ì„ ê³µìœ í•´ì£¼ì‹œë©´, ì •í™•í•œ ROIë¥¼ ì‚°ì¶œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    conv_manager.add_message("user", user_input, metadata={"type": "text"})
    
    # AI ì‘ë‹µ ìƒì„±
    context = conv_manager.get_context()
    history = conv_manager.get_formatted_history(for_llm=True)
    
    with st.spinner("ë¶„ì„ ì¤‘..."):
        time.sleep(1.0)
        ai_response = generate_ai_response(user_input, context, history)
    
    conv_manager.add_message("ai", ai_response)
    st.rerun()

# ============================================
# 7. ë¦¬ë“œ ì „í™˜ í¼
# ============================================
if conv_manager.is_ready_for_conversion() and conv_manager.get_context()['stage'] != 'complete':
    st.markdown("---")
    st.markdown('<p class="section-title">AI ì•„í‚¤í…ì²˜ ì„¤ê³„ ì œì•ˆì„œ ì‹ ì²­</p>', unsafe_allow_html=True)
    st.markdown("<p style='text-align:center; color:#94A3B8; font-size:14px;'>ë‹´ë‹¹ ì»¨ì„¤í„´íŠ¸ê°€ 24ì‹œê°„ ë‚´ ì—°ë½ë“œë¦½ë‹ˆë‹¤</p>", unsafe_allow_html=True)
    
    with st.form("lead_capture_form"):
        col1, col2 = st.columns(2)
        with col1:
            name = st.text_input("ì„±í•¨ / ì§í•¨", placeholder="í™ê¸¸ë™ / ëŒ€í‘œì´ì‚¬")
        with col2:
            contact = st.text_input("ì—°ë½ì²˜", placeholder="010-1234-5678")
ì•„ë˜ ê°„ë‹¨í•œ ì •ë³´ë§Œ ë‚¨ê²¨ì£¼ì‹œë©´ ë§ì¶¤ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."""

        company = st.text_input("ê¸°ì—…ëª… / ë³‘ì›ëª…", placeholder="ì˜ˆ: (ì£¼)ABCì»´í¼ë‹ˆ")
        urgency = st.selectbox("ë„ì… í¬ë§ ì‹œê¸°", URGENCY_OPTIONS)
        # íš¨ê³¼ ì˜ì‹¬
        elif any(word in user_lower for word in ['íš¨ê³¼', 'ì§„ì§œ', 'ì •ë§', 'ì˜ì‹¬']):
            case = CASE_STUDIES.get(context.get('user_type', 'hospital'))
            return f"""ë°ì´í„°ë¡œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

{case['title']} ë„ì… ì‚¬ë¡€:
- {case['result']}
- {case['data']}

ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"""

        submitted = st.form_submit_button("ì œì•ˆì„œ ì‹ ì²­", use_container_width=True)
        # ì‹œê°„ ë¶€ì¡±
        elif any(word in user_lower for word in ['ì‹œê°„', 'ë°”ì˜', 'ë‚˜ì¤‘']):
            return """ì´í•´í•©ë‹ˆë‹¤.

í•µì‹¬ë§Œ ë§ì”€ë“œë¦¬ë©´, í˜„ì¬ ë†“ì¹˜ëŠ” ê³ ê°ì˜ í‰ê·  30%ë¥¼ AIê°€ ìë™ ì „í™˜í•©ë‹ˆë‹¤.

ê°„ë‹¨í•œ ì—°ë½ì²˜ë§Œ ë‚¨ê²¨ì£¼ì‹œë©´ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë°œì†¡í•´ë“œë¦½ë‹ˆë‹¤."""

        if submitted:
            if not name or not contact:
                st.error("í•„ìˆ˜ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ë¦¬ë“œ ì €ì¥
                lead_data = {
                    'user_type': conv_manager.get_context().get('user_type', 'Unknown'),
                    'stage': 'Lead Converted',
                    'name': name,
                    'contact': contact,
                    'company': company,
                    'urgency': urgency,
                    'source': 'IMD_AI_Consultant'
                }
                
                success, message = lead_handler.save_lead(lead_data)
                
                if success:
                    # ì™„ë£Œ ë©”ì‹œì§€
                    completion_msg = f"""
### ì‹ ì²­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤
        # ê¸°ë³¸ ì‘ë‹µ
        else:
            return """í˜„ì¬ ì‚¬ì—…ì˜ í•µì‹¬ ê³¼ì œë¥¼ íŒŒì•…í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

**{name}ë‹˜**, ê°ì‚¬í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì¤‘ ê°€ì¥ ì‹œê¸‰í•œ ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?

ë‹´ë‹¹ ì»¨ì„¤í„´íŠ¸ê°€ **24ì‹œê°„ ë‚´**ë¡œ ì•„ë˜ ì—°ë½ì²˜ë¡œ ë§ì¶¤ ë¶„ì„ ë¦¬í¬íŠ¸ì™€ í•¨ê»˜ ì—°ë½ë“œë¦½ë‹ˆë‹¤.
1. ê´‘ê³  ëŒ€ë¹„ ë§¤ì¶œ íš¨ìœ¨
2. ê³ ê° ì „í™˜ìœ¨
3. ìš´ì˜ ì¸ë ¥ ë¶€ì¡±

**ì—°ë½ì²˜**: {contact}  
**í¬ë§ ì‹œê¸°**: {urgency}
êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œë©´ ë§ì¶¤ ì†”ë£¨ì…˜ì„ ì œì•ˆë“œë¦½ë‹ˆë‹¤."""
    
    def generate_initial_message(self) -> str:
        """ì²« ë©”ì‹œì§€"""
        return """ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ. <b>IMD ì•„í‚¤í…ì²˜ ê·¸ë£¹</b> ìˆ˜ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

---
ëŒ€í‘œë‹˜ ì‚¬ì—…ì˜ í•µì‹¬ ê³¼ì œë¥¼ íŒŒì•…í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

**ë‹¤ìŒ ë‹¨ê³„:**
1. 24ì‹œê°„ ë‚´: 1ì°¨ ì „í™” ìƒë‹´
2. 48ì‹œê°„ ë‚´: ë§ì¶¤ AI ì„¤ê³„ ì œì•ˆì„œ ë°œì†¡
3. 7ì¼ ë‚´: ì‹¤ì œ ë°ëª¨ ì‹œì—° (ì„ íƒ)
"""
                    conv_manager.add_message("ai", completion_msg)
                    conv_manager.update_stage('complete')
                    
                    st.success("ì‹ ì²­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error(f"ì˜¤ë¥˜: {message}")
í˜„ì¬ <b>ë§ˆì¼€íŒ… íˆ¬ì ëŒ€ë¹„ íš¨ìœ¨(ROAS)</b>ì— ë§Œì¡±í•˜ê³  ê³„ì‹­ë‹ˆê¹Œ?"""

# ============================================
# 8. ì™„ë£Œ í›„ ì•¡ì…˜
# ============================================
if conv_manager.get_context()['stage'] == 'complete':
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ìƒˆ ìƒë‹´ ì‹œì‘", use_container_width=True):
            conv_manager.reset_conversation()
            st.rerun()
    
    with col2:
        if st.button("ëŒ€í™” ìš”ì•½ ë³´ê¸°", use_container_width=True):
            with st.expander("ìƒë‹´ ìš”ì•½", expanded=True):
                st.markdown(conv_manager.get_summary())

# ============================================
# 9. ì‚¬ì´ë“œë°” (ê°„ì†Œí™”)
# ============================================
with st.sidebar:
    st.markdown(f"<h3 style='color:{COLOR_PRIMARY};'>IMD</h3>", unsafe_allow_html=True)
    st.markdown("<p style='color:#94A3B8; font-size:12px;'>AI Architecture Group</p>", unsafe_allow_html=True)
    
    st.markdown("---")
    
    # ì§„í–‰ë„
    trust = conv_manager.get_context()['trust_level']
    st.metric("ìƒë‹´ ì§„í–‰ë„", f"{trust}%")
    
    # ê°œë°œì ëª¨ë“œ (ê°„ì†Œí™”)
    if st.checkbox("ì‹œìŠ¤í…œ ì •ë³´"):
        st.json({
            "messages": len(conv_manager.get_history()),
            "stage": conv_manager.get_context()['stage'],
            "user_type": conv_manager.get_context().get('user_type', 'Unknown')
        })
def get_prompt_engine() -> PromptEngine:
    """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    if 'prompt_engine' not in st.session_state:
        st.session_state.prompt_engine = PromptEngine()
    return st.session_state.prompt_engine

# ============================================
# 10. í‘¸í„°
# ============================================
st.markdown("---")
st.markdown(
    f"""
    <div style='text-align:center; color:#64748B; font-size:11px; padding: 20px 0;'>
        <b style='color:{COLOR_PRIMARY};'>IMD Architecture Group</b><br>
        Enterprise AI Solutions | Powered by Gemini 2.0<br>
        Â© 2024 Reset Security. All rights reserved.
    </div>
    """,
    unsafe_allow_html=True
)

def generate_ai_response(user_input: str, context: Dict, history: str) -> str:
    """ë¹ ë¥¸ í˜¸ì¶œ"""
    engine = get_prompt_engine()
    return engine.generate_response(user_input, context, history)
