# prompt_engine.py
"""
IMD Sales Bot - AI Response Generation
Gemini ì „ìš© + Fallback ë°©ì§€ ê°•í™”
"""

import streamlit as st
import google.generativeai as genai
from typing import Dict, Optional
import time
from config import (
    SYSTEM_PROMPT,
    GEMINI_MODEL,
    GEMINI_TEMPERATURE,
    GEMINI_MAX_TOKENS,
    CASE_STUDIES,
    MAX_RETRY_ATTEMPTS
)

class PromptEngine:
    """AI ì‘ë‹µ ìƒì„± ì—”ì§„ (Fallback ë°©ì§€ ê°•í™”)"""
    
    def __init__(self):
        """Gemini API ì´ˆê¸°í™”"""
        self.model = None
        self.retry_count = 0
        self._init_gemini()
    
    def _init_gemini(self):
        """Gemini API ì„¤ì •"""
        try:
            if "GEMINI_API_KEY" not in st.secrets:
                st.error("âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                self.model = None
                return
            
            api_key = st.secrets["GEMINI_API_KEY"]
            
            if not api_key:
                st.error("âŒ API í‚¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                self.model = None
                return
            
            # Gemini ì„¤ì •
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(
                model_name=GEMINI_MODEL,
                generation_config={
                    "temperature": GEMINI_TEMPERATURE,
                    "max_output_tokens": GEMINI_MAX_TOKENS,
                }
            )
            
            # ì´ˆê¸°í™” ì„±ê³µ (ë¡œê·¸ë§Œ, í™”ë©´ í‘œì‹œ X)
            
        except Exception as e:
            st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.model = None
    
    def generate_response(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """
        AI ì‘ë‹µ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        """
        if not self.model:
            st.error("âš ï¸ AI ì‹œìŠ¤í…œì´ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
            return self._fallback_response(user_input, context)
        
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(MAX_RETRY_ATTEMPTS):
            try:
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                full_prompt = self._build_prompt(user_input, context, conversation_history)
                
                # Gemini í˜¸ì¶œ
                response = self.model.generate_content(full_prompt)
                
                # ì‘ë‹µ ê²€ì¦
                if not response or not response.text:
                    raise ValueError("ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                
                # ì‘ë‹µ í›„ì²˜ë¦¬
                ai_response = self._post_process_response(response.text.strip(), context)
                
                # ì„±ê³µ ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                self.retry_count = 0
                
                return ai_response
                
            except Exception as e:
                self.retry_count += 1
                error_msg = str(e)
                
                # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œ ì‹¤íŒ¨
                if attempt == MAX_RETRY_ATTEMPTS - 1:
                    st.error(f"âš ï¸ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRY_ATTEMPTS})")
                    st.error(f"ì˜¤ë¥˜: {error_msg}")
                    
                    # ì—ëŸ¬ íƒ€ì…ë³„ ì•ˆë‚´
                    if "quota" in error_msg.lower() or "rate" in error_msg.lower():
                        st.warning("ğŸ’¡ API ì‚¬ìš©ëŸ‰ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    elif "invalid" in error_msg.lower():
                        st.warning("ğŸ’¡ API í‚¤ ì˜¤ë¥˜. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
                    
                    return self._fallback_response(user_input, context)
                else:
                    # ì¬ì‹œë„
                    st.warning(f"ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{MAX_RETRY_ATTEMPTS})")
                    time.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        
        return self._fallback_response(user_input, context)
    
    def _build_prompt(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¡°ë¦½"""
        system_prompt = SYSTEM_PROMPT.format(
            user_type=context.get('user_type') or 'ë¯¸íŒŒì•…',
            pain_point=context.get('pain_point') or 'ë¯¸íŒŒì•…',
            stage=context.get('stage', 'initial'),
            trust_level=context.get('trust_level', 0)
        )
        
        # ë°˜ë°• ì‚¬í•­ ëŒ€ì‘
        if context.get('objections'):
            objection_guide = self._get_objection_handling(context['objections'])
            system_prompt += f"\n\n## í˜„ì¬ ê³ ê° ìš°ë ¤ì‚¬í•­\n{objection_guide}"
        
        # ì‚¬ë¡€ ì—°êµ¬ ì¶”ê°€
        if context.get('user_type') in CASE_STUDIES:
            case = CASE_STUDIES[context['user_type']]
            system_prompt += f"\n\n## ì œì‹œ ê°€ëŠ¥í•œ ì‹¤ì œ ì‚¬ë¡€\n- {case['title']}: {case['result']}\n- ì •ëŸ‰ ë°ì´í„°: {case['data']}"
        
        full_prompt = f"""{system_prompt}

---

## ìµœê·¼ ëŒ€í™” ë‚´ì—­
{conversation_history}

---

## ê³ ê°ì˜ ìµœì‹  ì…ë ¥
{user_input}

---

**ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ì‘ë‹µí•˜ì„¸ìš”. 2-4ë¬¸ì¥, ë°ì´í„° ê¸°ë°˜, ëª…í™•í•œ ì œì•ˆ.**
"""
        return full_prompt
    
    def _get_objection_handling(self, objections: list) -> str:
        """ë°˜ë°• ì‚¬í•­ ëŒ€ì‘ ì „ëµ"""
        strategies = {
            'skeptical': "â†’ ì •ëŸ‰ ë°ì´í„°ë¡œ ì¦ëª…. 'ë„ì… 6ê°œì›”, ì „í™˜ìœ¨ 32% ìƒìŠ¹' ê°™ì€ êµ¬ì²´ì  ìˆ˜ì¹˜ ì œì‹œ",
            'complexity': "â†’ 'ì´ˆê¸° ì…‹ì—… 3ì¼, ì§ì› êµìœ¡ 2ì‹œê°„' ê°™ì´ êµ¬ì²´ì  ì¼ì • ëª…ì‹œ",
            'price_sensitive': "â†’ ROI ì¤‘ì‹¬. 'ì›” 200ë§Œì› íˆ¬ì ì‹œ íšŒìˆ˜ ê¸°ê°„ 3ê°œì›”, ì—°ê°„ 2,400ë§Œì› ì¶”ê°€ ë§¤ì¶œ'"
        }
        
        guide = [strategies[obj] for obj in objections if obj in strategies]
        return "\n".join(guide) if guide else "ë°ì´í„° ê¸°ë°˜ í•´ê²°ì±… ì œì‹œ"
    
    def _post_process_response(self, response: str, context: Dict) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        import re
        
        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        response = response.replace('\n\n\n', '\n\n')
        
        # ë§ˆí¬ë‹¤ìš´ êµµê¸° ì²˜ë¦¬
        response = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', response)
        
        # ê¸¸ì´ ì œí•œ (600ì)
        if len(response) > 600:
            response = response[:580] + "\n\nì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?"
        
        # ê¸ˆì§€ ë‹¨ì–´ í•„í„°
        forbidden_words = {
            'LLM': 'AI ì‹œìŠ¤í…œ',
            'RAG': 'AI ê¸°ìˆ ',
            'API': 'ì‹œìŠ¤í…œ',
            'ë¨¸ì‹ ëŸ¬ë‹': 'AI',
            'ë”¥ëŸ¬ë‹': 'AI'
        }
        
        for word, replacement in forbidden_words.items():
            response = response.replace(word, replacement)
        
        # CTA ì¶”ê°€ (ì ì ˆí•œ íƒ€ì´ë°)
        if context.get('trust_level', 0) >= 50 and 'ì„¤ê³„' not in response.lower():
            response += "\n\nì•„ë˜ ì •ë³´ë¥¼ ë‚¨ê²¨ì£¼ì‹œë©´ ë§ì¶¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ë°œì†¡í•´ë“œë¦½ë‹ˆë‹¤."
        
        return response
    
    def _fallback_response(self, user_input: str, context: Dict) -> str:
        """Fallback ì‘ë‹µ"""
        user_lower = user_input.lower()
        
        # ê°€ê²© ë¬¸ì˜
        if any(word in user_lower for word in ['ê°€ê²©', 'ë¹„ìš©', 'ì–¼ë§ˆ', 'ìš”ê¸ˆ']):
            return """ì›ì¥ë‹˜, íˆ¬ì ê¸ˆì•¡ë³´ë‹¤ ì¤‘ìš”í•œ ê²ƒì€ íšŒìˆ˜ ê¸°ê°„ì…ë‹ˆë‹¤.

ì €í¬ ë³‘ì› ê³ ê°ì‚¬ í‰ê·  íšŒìˆ˜ ê¸°ê°„ì€ 3ê°œì›”ì…ë‹ˆë‹¤.
ì˜ˆ: ì›” 200ë§Œì› íˆ¬ì â†’ ì•¼ê°„ ì˜ˆì•½ ì¦ê°€ë¡œ ì›” 500ë§Œì› ì¶”ê°€ ë§¤ì¶œ

ì—°ë½ì²˜ë§Œ ë‚¨ê²¨ì£¼ì‹œë©´ ì›ì¥ë‹˜ ë³‘ì›ì— ë§ëŠ” ì •í™•í•œ ROIë¥¼ ì‚°ì¶œí•´ë“œë¦½ë‹ˆë‹¤."""
        
        # íš¨ê³¼ ì˜ì‹¬
        elif any(word in user_lower for word in ['íš¨ê³¼', 'ì§„ì§œ', 'ì •ë§', 'ì˜ì‹¬']):
            return """ì›ì¥ë‹˜, ë°ì´í„°ë¡œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ì„œìš¸ Aì„±í˜•ì™¸ê³¼ ì‹¤ì œ ì‚¬ë¡€:**
- ë„ì… ì „: ì•¼ê°„ ë¬¸ì˜ ì‘ë‹µë¥  0%
- ë„ì… í›„: ì•¼ê°„ ì‘ë‹µë¥  100% â†’ ì˜ˆì•½ 32% ì¦ê°€
- ì›” ì¶”ê°€ ë§¤ì¶œ: 450ë§Œì›

ì§€ê¸ˆ ì €ì™€ ëŒ€í™”í•˜ì‹œëŠ” ì´ AIë¥¼ ì›ì¥ë‹˜ ë³‘ì›ì—ë„ ì„¤ì¹˜í•´ë“œë¦½ë‹ˆë‹¤.
ë¬´ë£Œ ë°ëª¨ë¥¼ ì²´í—˜í•´ë³´ì‹œê² ìŠµë‹ˆê¹Œ?"""
        
        # ì‹œê°„ ë¶€ì¡±
        elif any(word in user_lower for word in ['ì‹œê°„', 'ë°”ì˜', 'ë‚˜ì¤‘']):
            return """ì›ì¥ë‹˜, ë°”ì˜ì‹  ì¤‘ì— ì‹œê°„ ë‚´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.

í•µì‹¬ë§Œ ë§ì”€ë“œë¦¬ë©´:
ë°¤ 11ì‹œì— í™ˆí˜ì´ì§€ ë“¤ì–´ì˜¨ í™˜ì, ë‚´ì¼ ì•„ì¹¨ê¹Œì§€ ê¸°ë‹¤ë¦¬ë©´ ì´ë¯¸ ë‹¤ë¥¸ ë³‘ì› ì˜ˆì•½í–ˆìŠµë‹ˆë‹¤.

ì´ AI í•˜ë‚˜ë©´ 24ì‹œê°„ ë¬´ì¸ ìƒë‹´ìœ¼ë¡œ ì•¼ê°„ ë¬¸ì˜ 100% ì˜ˆì•½ ì „í™˜ë©ë‹ˆë‹¤.

ì—°ë½ì²˜ë§Œ ë‚¨ê²¨ì£¼ì‹œë©´ ë¬´ë£Œ ë°ëª¨ ë§í¬ ë³´ë‚´ë“œë¦½ë‹ˆë‹¤."""
        
        # ê¸°ë³¸ ì‘ë‹µ
        else:
            return """ì›ì¥ë‹˜, í˜„ì¬ ê°€ì¥ ë‹µë‹µí•œ ë¶€ë¶„ì´ ë¬´ì—‡ì¸ê°€ìš”?

1ï¸âƒ£ ê´‘ê³ ë¹„ëŠ” ì“°ëŠ”ë° ì˜ˆì•½ì´ ì•ˆ ë¼ì„œ?
2ï¸âƒ£ ì•¼ê°„/ì£¼ë§ ë¬¸ì˜ê°€ ë§ì€ë° ë†“ì³ì„œ?
3ï¸âƒ£ ìƒë‹´ì‚¬ ì¸ê±´ë¹„ê°€ ë¶€ë‹´ë¼ì„œ?

í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì‹œë©´ ë§ì¶¤ ì†”ë£¨ì…˜ì„ ì œì•ˆë“œë¦½ë‹ˆë‹¤."""
    
    def generate_initial_message(self) -> str:
        """ì²« ë©”ì‹œì§€"""
        return """ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ. <b>IMD ì•„í‚¤í…ì²˜ ê·¸ë£¹</b> ìˆ˜ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì›ì¥ë‹˜, ì†”ì§íˆ ì—¬ì­¤ë³´ê² ìŠµë‹ˆë‹¤.

ì§€ê¸ˆ <b>í™ˆí˜ì´ì§€ ë§ˆì¼€íŒ… ë¹„ìš© ëŒ€ë¹„ ì˜ˆì•½ ì „í™˜ìœ¨</b>ì— ë§Œì¡±í•˜ê³  ê³„ì‹­ë‹ˆê¹Œ?"""


def get_prompt_engine() -> PromptEngine:
    """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    if 'prompt_engine' not in st.session_state:
        st.session_state.prompt_engine = PromptEngine()
    return st.session_state.prompt_engine


def generate_ai_response(user_input: str, context: Dict, history: str) -> str:
    """ë¹ ë¥¸ í˜¸ì¶œ"""
    engine = get_prompt_engine()
    return engine.generate_response(user_input, context, history)
