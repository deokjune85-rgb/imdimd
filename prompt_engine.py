# prompt_engine.py
"""
IMD Sales Bot - AI Response Generation
Gemini API ì „ìš© (ë””ë²„ê¹… ê°•í™”)
"""

import streamlit as st
import google.generativeai as genai
from typing import Dict, Optional
from config import (
    SYSTEM_PROMPT,
    GEMINI_MODEL,
    GEMINI_TEMPERATURE,
    GEMINI_MAX_TOKENS,
    CASE_STUDIES
)

class PromptEngine:
    """AI ì‘ë‹µ ìƒì„± ì—”ì§„ (Gemini ì „ìš©)"""
    
    def __init__(self):
        """Gemini API ì´ˆê¸°í™”"""
        self.model = None
        self._init_gemini()
    
    def _init_gemini(self):
        """Gemini API ì„¤ì •"""
        try:
            # Secrets í™•ì¸
            if "GEMINI_API_KEY" not in st.secrets:
                st.error("âŒ st.secretsì— 'GEMINI_API_KEY'ê°€ ì—†ìŠµë‹ˆë‹¤!")
                st.info("Streamlit Cloud > Settings > Secretsì— ì¶”ê°€í•˜ì„¸ìš”.")
                self.model = None
                return
            
            api_key = st.secrets["GEMINI_API_KEY"]
            
            if not api_key or api_key == "":
                st.error("âŒ GEMINI_API_KEYê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                self.model = None
                return
            
            # API í‚¤ ìœ íš¨ì„± í‘œì‹œ (ì²˜ìŒ 3ê¸€ìë§Œ)
            st.success(f"âœ… Gemini API í‚¤ ê°ì§€ë¨: {api_key[:8]}...")
            
            # Gemini ì„¤ì •
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(
                model_name=GEMINI_MODEL,
                generation_config={
                    "temperature": GEMINI_TEMPERATURE,
                    "max_output_tokens": GEMINI_MAX_TOKENS,
                }
            )
            
            st.success(f"âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {GEMINI_MODEL}")
            
        except Exception as e:
            st.error(f"âŒ Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")
            self.model = None
    
    def generate_response(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ AI ì‘ë‹µ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (user_type, pain_point ë“±)
            conversation_history: ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬
        
        Returns:
            AI ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        # ë””ë²„ê·¸ ì •ë³´
        st.info(f"ğŸ”§ DEBUG: ëª¨ë¸ ì—°ê²° ìƒíƒœ = {'ì—°ê²°ë¨' if self.model else 'ë¯¸ì—°ê²°'}")
        
        if not self.model:
            st.warning("âš ï¸ Gemini ë¯¸ì—°ê²° - Fallback ì‘ë‹µ ì‚¬ìš©")
            return self._fallback_response(user_input, context)
        
        try:
            st.info("ğŸ”§ DEBUG: í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            
            # ë™ì  System Prompt ìƒì„±
            full_prompt = self._build_prompt(user_input, context, conversation_history)
            
            st.info(f"ğŸ”§ DEBUG: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ = {len(full_prompt)} ê¸€ì")
            st.info("ğŸ”§ DEBUG: Gemini API í˜¸ì¶œ ì‹œì‘...")
            
            # Gemini API í˜¸ì¶œ
            response = self.model.generate_content(full_prompt)
            
            st.success("ğŸ”§ DEBUG: Gemini ì‘ë‹µ ë°›ìŒ!")
            st.info(f"ğŸ”§ DEBUG: ì›ë³¸ ì‘ë‹µ ê¸¸ì´ = {len(response.text)} ê¸€ì")
            
            # ì‘ë‹µ í›„ì²˜ë¦¬
            ai_response = self._post_process_response(response.text.strip(), context)
            
            st.success(f"ğŸ”§ DEBUG: ìµœì¢… ì‘ë‹µ ê¸¸ì´ = {len(ai_response)} ê¸€ì")
            
            # ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 100ì)
            st.code(ai_response[:100] + "...", language="text")
            
            return ai_response
            
        except Exception as e:
            st.error(f"âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ìƒì„¸ ì—ëŸ¬ ë¡œê·¸
            import traceback
            error_detail = traceback.format_exc()
            st.code(error_detail, language="python")
            
            # ì—ëŸ¬ íƒ€ì…ë³„ ì•ˆë‚´
            error_str = str(e).lower()
            if "quota" in error_str or "rate" in error_str:
                st.warning("ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼! ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            elif "invalid" in error_str:
                st.warning("ğŸ’¡ API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Secrets í™•ì¸í•˜ì„¸ìš”.")
            
            return self._fallback_response(user_input, context)
    
    def _build_prompt(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """
        ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            context: ì»¨í…ìŠ¤íŠ¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
        
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # System Promptì— ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        system_prompt = SYSTEM_PROMPT.format(
            user_type=context.get('user_type') or 'ë¯¸íŒŒì•…',
            pain_point=context.get('pain_point') or 'ë¯¸íŒŒì•…',
            stage=context.get('stage', 'initial'),
            trust_level=context.get('trust_level', 0)
        )
        
        # ë°˜ë°• ì‚¬í•­ ëŒ€ì‘ ì „ëµ ì¶”ê°€
        if context.get('objections'):
            objection_guide = self._get_objection_handling(context['objections'])
            system_prompt += f"\n\n## í˜„ì¬ ê³ ê° ìš°ë ¤ì‚¬í•­\n{objection_guide}"
        
        # ì‚¬ë¡€ ì—°êµ¬ ì¶”ê°€ (ì—…ì¢…ë³„)
        if context.get('user_type') in CASE_STUDIES:
            case = CASE_STUDIES[context['user_type']]
            system_prompt += f"\n\n## ì œì‹œí•  ìˆ˜ ìˆëŠ” ì‹¤ì œ ì‚¬ë¡€\n- {case['title']}: {case['result']}\n- ê³ ê° í›„ê¸°: \"{case['quote']}\""
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        full_prompt = f"""{system_prompt}

---

## ìµœê·¼ ëŒ€í™” ë‚´ì—­
{conversation_history}

---

## ê³ ê°ì˜ ìµœì‹  ì…ë ¥
ê³ ê°: {user_input}

---

**ìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì§€ê¸ˆ ì¦‰ì‹œ ì‘ë‹µí•˜ì„¸ìš”.**
ì‘ë‹µì€ 3-5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
í•µì‹¬ ë©”ì‹œì§€ í•˜ë‚˜ì— ì§‘ì¤‘í•˜ì„¸ìš”.
"""
        return full_prompt
    
    def _get_objection_handling(self, objections: list) -> str:
        """
        ë°˜ë°• ì‚¬í•­ë³„ ëŒ€ì‘ ì „ëµ
        
        Args:
            objections: ìš°ë ¤ ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ëŒ€ì‘ ê°€ì´ë“œ
        """
        strategies = {
            'skeptical': "â†’ ì‹¤ì œ ì‚¬ë¡€ì™€ êµ¬ì²´ì  ìˆ˜ì¹˜ë¡œ ì¦ëª…í•˜ì„¸ìš”. 'ì§€ê¸ˆ ì €ì™€ ëŒ€í™”í•˜ëŠ” ê²ƒì²˜ëŸ¼...' í”„ë ˆì„ ì‚¬ìš©",
            'complexity': "â†’ 'ì„¤ì¹˜ 3ì¼, êµìœ¡ 1ì‹œê°„ì´ë©´ ë' ê°™ì´ êµ¬ì²´ì  ì¼ì • ì œì‹œ",
            'price_sensitive': "â†’ ê°€ê²©ì´ ì•„ë‹Œ ROIë¡œ ì „í™˜. 'ì›” 200ë§Œì› íˆ¬ìë¡œ ì›” 1000ë§Œì› ì¶”ê°€ ë§¤ì¶œ' ì‹ìœ¼ë¡œ ì œì‹œ"
        }
        
        guide = []
        for obj in objections:
            if obj in strategies:
                guide.append(strategies[obj])
        
        return "\n".join(guide) if guide else "ê³ ê°ì˜ ìš°ë ¤ë¥¼ ê³µê°í•˜ê³  êµ¬ì²´ì  í•´ê²°ì±… ì œì‹œ"
    
    def _post_process_response(self, response: str, context: Dict) -> str:
        """
        AI ì‘ë‹µ í›„ì²˜ë¦¬ (í¬ë§·íŒ…, ì•ˆì „ì¥ì¹˜)
        
        Args:
            response: ì›ë³¸ ì‘ë‹µ
            context: ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            ì²˜ë¦¬ëœ ì‘ë‹µ
        """
        # 1. ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
        response = response.replace('\n\n\n', '\n\n')
        
        # 2. ë§ˆí¬ë‹¤ìš´ êµµê¸° ì²˜ë¦¬ (** â†’ <b>)
        import re
        response = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', response)
        
        # 3. ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (500ì ì œí•œ)
        if len(response) > 500:
            response = response[:480] + "...\n\nê³„ì† ë“¤ì–´ë³´ì‹œê² ì–´ìš”?"
        
        # 4. ê¸ˆì§€ ë‹¨ì–´ í•„í„°ë§
        forbidden = ['LLM', 'RAG', 'API', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹']
        for word in forbidden:
            if word in response:
                response = response.replace(word, 'AI ê¸°ìˆ ')
        
        # 5. CTA ìë™ ì¶”ê°€ (ì „í™˜ íƒ€ì´ë°)
        if context.get('trust_level', 0) >= 60 and 'ë¬´ë£Œ' not in response:
            response += "\n\nğŸ’¡ ì§€ê¸ˆ ë¬´ë£Œ ì„¤ê³„ë„ë¼ë„ ë°›ì•„ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        
        return response
    
    def _fallback_response(self, user_input: str, context: Dict) -> str:
        """
        API ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ (ê·œì¹™ ê¸°ë°˜)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            context: ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            í´ë°± ì‘ë‹µ
        """
        user_lower = user_input.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë‹¨ìˆœ ì‘ë‹µ
        if any(word in user_lower for word in ['ê°€ê²©', 'ë¹„ìš©', 'ì–¼ë§ˆ']):
            return """ëŒ€í‘œë‹˜, ì†”ì§íˆ ë§ì”€ë“œë¦¬ë©´ 'ê°€ê²©'ë³´ë‹¤ ì¤‘ìš”í•œ ê²Œ ìˆìŠµë‹ˆë‹¤.
            
ì§€ê¸ˆ í™ˆí˜ì´ì§€ ë°©ë¬¸ì 100ëª… ì¤‘ ëª‡ ëª…ì´ êµ¬ë§¤/ì˜ˆì•½í•˜ì‹œë‚˜ìš”?
ë§Œì•½ 2%ë¼ë©´, AIë¡œ 3%ë§Œ ì˜¬ë ¤ë„ ì›”ë§¤ì¶œì´ 50% ëŠ˜ì–´ë‚©ë‹ˆë‹¤.

íˆ¬ì ëŒ€ë¹„ ìˆ˜ìµ(ROI)ì„ ë¨¼ì € ê³„ì‚°í•´ë³´ì‹œê² ì–´ìš”?"""
        
        elif any(word in user_lower for word in ['íš¨ê³¼', 'ì§„ì§œ', 'ì •ë§']):
            case = CASE_STUDIES.get(context.get('user_type', 'hospital'))
            return f"""ë‹¹ì—°íˆ ì˜ì‹¬ìŠ¤ëŸ¬ìš°ì‹¤ ê²ë‹ˆë‹¤. ê·¼ë° ëŒ€í‘œë‹˜, ì§€ê¸ˆ ì €ì™€ ëŒ€í™”í•˜ì‹œë©´ì„œ ëŠë¼ì…¨ë‚˜ìš”?

ì œê°€ ì‚¬ëŒì²˜ëŸ¼ ëŒ€ë‹µí•œë‹¤ëŠ” ê±¸?

ì‹¤ì œë¡œ <b>{case['title']}</b>ëŠ” ë„ì… í›„ <b>{case['result']}</b> ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

"{case['quote']}"

ì‹¤ì œ ì‚¬ë¡€ë¥¼ ë” ë³´ì‹œê² ì–´ìš”?"""
        
        elif any(word in user_lower for word in ['ì‹œê°„', 'ë°”ì˜', 'ë‚˜ì¤‘']):
            return """ëŒ€í‘œë‹˜, ë”± 2ë¶„ë§Œ íˆ¬ìí•˜ì„¸ìš”.
            
ì§€ê¸ˆ ê²½ìŸì‚¬ë“¤ì€ AIë¡œ ì•¼ê°„/ì£¼ë§ ê³ ê°ê¹Œì§€ ì¡ê³  ìˆìŠµë‹ˆë‹¤.
ëŒ€í‘œë‹˜ì´ 'ë‚˜ì¤‘ì—'ë¥¼ ê³ ë¯¼í•˜ëŠ” ì‚¬ì´, ê³ ê°ì€ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ê°‘ë‹ˆë‹¤.

ë¬´ë£Œ ì„¤ê³„ë„ëŠ” ë°›ì•„ë‘ì‹œê³  ê²€í† í•˜ì…”ë„ ë©ë‹ˆë‹¤. ì†í•´ ë³¼ ê²Œ ì—†ì–ì•„ìš”?"""
        
        else:
            return """ë§ì”€ ê°ì‚¬í•©ë‹ˆë‹¤. ë” ìì„¸íˆ ë“£ê³  ì‹¶ì€ë°ìš”,

ì§€ê¸ˆ ê°€ì¥ ë‹µë‹µí•œ ë¶€ë¶„ì´ ë­”ê°€ìš”?
1ï¸âƒ£ ê´‘ê³ ë¹„ ëŒ€ë¹„ ë§¤ì¶œì´ ì•ˆ ë‚˜ì™€ì„œ?
2ï¸âƒ£ ê³ ê°ì´ ë¬¸ì˜ë§Œ í•˜ê³  êµ¬ë§¤/ì˜ˆì•½ ì•ˆ í•´ì„œ?
3ï¸âƒ£ ì§ì›ë“¤ì´ ì•¼ê·¼í•´ë„ ëŒ€ì‘ì´ ì•ˆ ë¼ì„œ?

í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”."""
    
    def generate_initial_message(self) -> str:
        """
        ì²« ì¸ì‚¬ ë©”ì‹œì§€ ìƒì„± (ê³ ì •)
        
        Returns:
            ì²« ë©”ì‹œì§€
        """
        return """ë°˜ê°‘ìŠµë‹ˆë‹¤. <b>IMD ìˆ˜ì„ ì•„í‚¤í…íŠ¸ AI</b>ì…ë‹ˆë‹¤.

ëŒ€í‘œë‹˜, ì†”ì§íˆ ë§ì”€ë“œë¦¬ì£ .

ì§€ê¸ˆ <b>ë§ˆì¼€íŒ… ë¹„ìš© ëŒ€ë¹„ íš¨ìœ¨(ROAS)</b>, ë§Œì¡±í•˜ì‹œë‚˜ìš”?"""


# ============================================
# í¸ì˜ í•¨ìˆ˜
# ============================================
def get_prompt_engine() -> PromptEngine:
    """PromptEngine ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    if 'prompt_engine' not in st.session_state:
        st.session_state.prompt_engine = PromptEngine()
    return st.session_state.prompt_engine


def generate_ai_response(user_input: str, context: Dict, history: str) -> str:
    """
    ë¹ ë¥¸ AI ì‘ë‹µ ìƒì„± (ì•±ì—ì„œ ë°”ë¡œ í˜¸ì¶œìš©)
    
    Args:
        user_input: ì‚¬ìš©ì ë©”ì‹œì§€
        context: ì»¨í…ìŠ¤íŠ¸
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬
    
    Returns:
        AI ì‘ë‹µ
    """
    engine = get_prompt_engine()
    return engine.generate_response(user_input, context, history)
