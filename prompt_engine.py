# prompt_engine.py
"""
IMD Sales Bot - AI Response Generation
Multi-LLM ì§€ì› (Gemini, Groq, OpenRouter)
"""

import streamlit as st
from typing import Dict, Optional
from config import (
    SYSTEM_PROMPT,
    GEMINI_MODEL,
    GEMINI_TEMPERATURE,
    GEMINI_MAX_TOKENS,
    CASE_STUDIES
)

# LLM ì„ íƒì— ë”°ë¥¸ import
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except:
    GEMINI_AVAILABLE = False

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except:
    GROQ_AVAILABLE = False

try:
    import requests
    OPENROUTER_AVAILABLE = True
except:
    OPENROUTER_AVAILABLE = False

class PromptEngine:
    """AI ì‘ë‹µ ìƒì„± ì—”ì§„ (Gemini, Groq, OpenRouter ì§€ì›)"""
    
    def __init__(self):
        """LLM API ì´ˆê¸°í™”"""
        self.model = None
        self.llm_type = None  # 'gemini', 'groq', 'openrouter'
        self._init_llm()
    
    def _init_llm(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ìë™ ê°ì§€ ë° ì´ˆê¸°í™”"""
        
        # 0ìˆœìœ„: OpenRouter (ê°€ì¥ ìœ ì—°í•¨, ì—¬ëŸ¬ ëª¨ë¸)
        if "OPENROUTER_API_KEY" in st.secrets and OPENROUTER_AVAILABLE:
            try:
                self.api_key = st.secrets["OPENROUTER_API_KEY"]
                # ëª¨ë¸ ì„ íƒ (Secretsì—ì„œ ì§€ì • ê°€ëŠ¥)
                self.model_name = st.secrets.get(
                    "OPENROUTER_MODEL", 
                    "google/gemini-2.0-flash-exp:free"  # ê¸°ë³¸ê°’: Gemini ë¬´ë£Œ
                )
                self.model = "openrouter"  # í”Œë˜ê·¸
                self.llm_type = "openrouter"
                st.success(f"âœ… OpenRouter ì—°ê²° ì™„ë£Œ (ëª¨ë¸: {self.model_name})")
                return
            except Exception as e:
                st.warning(f"OpenRouter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # 1ìˆœìœ„: Groq (ê°€ì¥ ë¹ ë¥´ê³  ë¬´ë£Œ)
        if "GROQ_API_KEY" in st.secrets and GROQ_AVAILABLE:
            try:
                api_key = st.secrets["GROQ_API_KEY"]
                self.model = Groq(api_key=api_key)
                self.llm_type = "groq"
                st.success(f"âœ… Groq API ì—°ê²° ì™„ë£Œ (ì´ˆê³ ì† ëª¨ë“œ)")
                return
            except Exception as e:
                st.warning(f"Groq ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # 2ìˆœìœ„: Gemini
        if "GEMINI_API_KEY" in st.secrets and GEMINI_AVAILABLE:
            try:
                api_key = st.secrets["GEMINI_API_KEY"]
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel(
                    model_name=GEMINI_MODEL,
                    generation_config={
                        "temperature": GEMINI_TEMPERATURE,
                        "max_output_tokens": GEMINI_MAX_TOKENS,
                    }
                )
                self.llm_type = "gemini"
                st.success(f"âœ… Gemini API ì—°ê²° ì™„ë£Œ")
                return
            except Exception as e:
                st.warning(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ëª¨ë‘ ì‹¤íŒ¨
        st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM APIê°€ ì—†ìŠµë‹ˆë‹¤!")
        st.info("""
        **Secretsì— ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:**
        
        1. OpenRouter (ì¶”ì²œ, ë‹¤ì–‘í•œ ëª¨ë¸):
           ```
           OPENROUTER_API_KEY = "sk-or-v1-..."
           OPENROUTER_MODEL = "google/gemini-2.0-flash-exp:free"
           ```
           ë°œê¸‰: https://openrouter.ai/keys
        
        2. Groq (ë¹ ë¦„, ë¬´ë£Œ):
           ```
           GROQ_API_KEY = "gsk_..."
           ```
           ë°œê¸‰: https://console.groq.com/keys
        
        3. Gemini:
           ```
           GEMINI_API_KEY = "AIza..."
           ```
           ë°œê¸‰: https://aistudio.google.com/apikey
        """)
        self.model = None
        self.llm_type = None
    
    def generate_response(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ AI ì‘ë‹µ ìƒì„±
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (user_type, pain_point ë“±)
            conversation_history: ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬
        
        Returns:
            AI ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        if not self.model:
            st.warning("âš ï¸ LLM ë¯¸ì—°ê²° - Fallback ì‘ë‹µ ì‚¬ìš©")
            return self._fallback_response(user_input, context)
        
        try:
            # ë””ë²„ê·¸: LLM íƒ€ì… í™•ì¸
            st.info(f"ğŸ”§ DEBUG: LLM íƒ€ì… = {self.llm_type}")
            
            # ë™ì  System Prompt ìƒì„±
            full_prompt = self._build_prompt(user_input, context, conversation_history)
            st.info(f"ğŸ”§ DEBUG: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ = {len(full_prompt)} ê¸€ì")
            
            # LLMë³„ í˜¸ì¶œ ë°©ì‹
            if self.llm_type == "openrouter":
                st.info("ğŸ”§ DEBUG: OpenRouter í˜¸ì¶œ ì¤‘...")
                response = self._call_openrouter(full_prompt)
            elif self.llm_type == "groq":
                st.info("ğŸ”§ DEBUG: Groq í˜¸ì¶œ ì¤‘...")
                response = self._call_groq(full_prompt)
            elif self.llm_type == "gemini":
                st.info("ğŸ”§ DEBUG: Gemini í˜¸ì¶œ ì¤‘...")
                response = self._call_gemini(full_prompt)
            else:
                st.error(f"ğŸ”§ DEBUG: ì•Œ ìˆ˜ ì—†ëŠ” LLM íƒ€ì…: {self.llm_type}")
                return self._fallback_response(user_input, context)
            
            st.success(f"ğŸ”§ DEBUG: AI ì‘ë‹µ ë°›ìŒ (ê¸¸ì´: {len(response)} ê¸€ì)")
            
            # ì‘ë‹µ í›„ì²˜ë¦¬
            ai_response = self._post_process_response(response, context)
            st.success(f"ğŸ”§ DEBUG: í›„ì²˜ë¦¬ ì™„ë£Œ (ìµœì¢… ê¸¸ì´: {len(ai_response)} ê¸€ì)")
            
            return ai_response
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            st.error(f"âš ï¸ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            st.code(error_detail, language="python")
            return self._fallback_response(user_input, context)
    
    def _call_openrouter(self, prompt: str) -> str:
        """OpenRouter API í˜¸ì¶œ"""
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "HTTP-Referer": "https://imd-sales-bot.streamlit.app",  # ì„ íƒì‚¬í•­
                "X-Title": "IMD Sales Bot",  # ì„ íƒì‚¬í•­
            },
            json={
                "model": self.model_name,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.85,
                "max_tokens": 1000,
            }
        )
        
        if response.status_code != 200:
            raise Exception(f"OpenRouter API ì˜¤ë¥˜: {response.status_code} - {response.text}")
        
        result = response.json()
        return result["choices"][0]["message"]["content"]
    
    def _call_groq(self, prompt: str) -> str:
        """Groq API í˜¸ì¶œ"""
        chat_completion = self.model.chat.completions.create(
            messages=[
                {"role": "user", "content": prompt}
            ],
            model="llama-3.1-70b-versatile",  # ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ë¬´ë£Œ ëª¨ë¸
            temperature=0.85,
            max_tokens=1000,
        )
        return chat_completion.choices[0].message.content
    
    def _call_gemini(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ"""
        response = self.model.generate_content(prompt)
        return response.text
    
    def _build_prompt(
        self,
        user_input: str,
        context: Dict,
        conversation_history: str
    ) -> str:
        """
        ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            context: ì»¨í…ìŠ¤íŠ¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
        
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # System Promptì— ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
        system_prompt = SYSTEM_PROMPT.format(
            user_type=context.get('user_type') or 'ë¯¸íŒŒì•…',
            pain_point=context.get('pain_point') or 'ë¯¸íŒŒì•…',
            stage=context.get('stage', 'initial'),
            trust_level=context.get('trust_level', 0)
        )
        
        # ë°˜ë°• ì‚¬í•­ ëŒ€ì‘ ì „ëµ ì¶”ê°€
        if context.get('objections'):
            objection_guide = self._get_objection_handling(context['objections'])
            system_prompt += f"\n\n## í˜„ì¬ ê³ ê° ìš°ë ¤ì‚¬í•­\n{objection_guide}"
        
        # ì‚¬ë¡€ ì—°êµ¬ ì¶”ê°€ (ì—…ì¢…ë³„)
        if context.get('user_type') in CASE_STUDIES:
            case = CASE_STUDIES[context['user_type']]
            system_prompt += f"\n\n## ì œì‹œí•  ìˆ˜ ìˆëŠ” ì‹¤ì œ ì‚¬ë¡€\n- {case['title']}: {case['result']}\n- ê³ ê° í›„ê¸°: \"{case['quote']}\""
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        full_prompt = f"""{system_prompt}

---

## ìµœê·¼ ëŒ€í™” ë‚´ì—­
{conversation_history}

---

## ê³ ê°ì˜ ìµœì‹  ì…ë ¥
ê³ ê°: {user_input}

---

**ìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì§€ê¸ˆ ì¦‰ì‹œ ì‘ë‹µí•˜ì„¸ìš”.**
ì‘ë‹µì€ 3-5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
í•µì‹¬ ë©”ì‹œì§€ í•˜ë‚˜ì— ì§‘ì¤‘í•˜ì„¸ìš”.
"""
        return full_prompt
    
    def _get_objection_handling(self, objections: list) -> str:
        """
        ë°˜ë°• ì‚¬í•­ë³„ ëŒ€ì‘ ì „ëµ
        
        Args:
            objections: ìš°ë ¤ ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ëŒ€ì‘ ê°€ì´ë“œ
        """
        strategies = {
            'skeptical': "â†’ ì‹¤ì œ ì‚¬ë¡€ì™€ êµ¬ì²´ì  ìˆ˜ì¹˜ë¡œ ì¦ëª…í•˜ì„¸ìš”. 'ì§€ê¸ˆ ì €ì™€ ëŒ€í™”í•˜ëŠ” ê²ƒì²˜ëŸ¼...' í”„ë ˆì„ ì‚¬ìš©",
            'complexity': "â†’ 'ì„¤ì¹˜ 3ì¼, êµìœ¡ 1ì‹œê°„ì´ë©´ ë' ê°™ì´ êµ¬ì²´ì  ì¼ì • ì œì‹œ",
            'price_sensitive': "â†’ ê°€ê²©ì´ ì•„ë‹Œ ROIë¡œ ì „í™˜. 'ì›” 200ë§Œì› íˆ¬ìë¡œ ì›” 1000ë§Œì› ì¶”ê°€ ë§¤ì¶œ' ì‹ìœ¼ë¡œ ì œì‹œ"
        }
        
        guide = []
        for obj in objections:
            if obj in strategies:
                guide.append(strategies[obj])
        
        return "\n".join(guide) if guide else "ê³ ê°ì˜ ìš°ë ¤ë¥¼ ê³µê°í•˜ê³  êµ¬ì²´ì  í•´ê²°ì±… ì œì‹œ"
    
    def _post_process_response(self, response: str, context: Dict) -> str:
        """
        AI ì‘ë‹µ í›„ì²˜ë¦¬ (í¬ë§·íŒ…, ì•ˆì „ì¥ì¹˜)
        
        Args:
            response: ì›ë³¸ ì‘ë‹µ
            context: ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            ì²˜ë¦¬ëœ ì‘ë‹µ
        """
        # 1. ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
        response = response.replace('\n\n\n', '\n\n')
        
        # 2. ë§ˆí¬ë‹¤ìš´ êµµê¸° ì²˜ë¦¬ (** â†’ <b>)
        response = response.replace('**', '<b>', 1).replace('**', '</b>', 1)
        
        # 3. ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (500ì ì œí•œ)
        if len(response) > 500:
            response = response[:480] + "...\n\nê³„ì† ë“¤ì–´ë³´ì‹œê² ì–´ìš”?"
        
        # 4. ê¸ˆì§€ ë‹¨ì–´ í•„í„°ë§
        forbidden = ['LLM', 'RAG', 'API', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹']
        for word in forbidden:
            if word in response:
                response = response.replace(word, 'AI ê¸°ìˆ ')
        
        # 5. CTA ìë™ ì¶”ê°€ (ì „í™˜ íƒ€ì´ë°)
        if context.get('trust_level', 0) >= 60 and 'ë¬´ë£Œ' not in response:
            response += "\n\nğŸ’¡ ì§€ê¸ˆ ë¬´ë£Œ ì„¤ê³„ë„ë¼ë„ ë°›ì•„ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        
        return response
    
    def _fallback_response(self, user_input: str, context: Dict) -> str:
        """
        API ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ (ê·œì¹™ ê¸°ë°˜)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            context: ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            í´ë°± ì‘ë‹µ
        """
        user_lower = user_input.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë‹¨ìˆœ ì‘ë‹µ
        if any(word in user_lower for word in ['ê°€ê²©', 'ë¹„ìš©', 'ì–¼ë§ˆ']):
            return """ëŒ€í‘œë‹˜, ì†”ì§íˆ ë§ì”€ë“œë¦¬ë©´ 'ê°€ê²©'ë³´ë‹¤ ì¤‘ìš”í•œ ê²Œ ìˆìŠµë‹ˆë‹¤.
            
ì§€ê¸ˆ í™ˆí˜ì´ì§€ ë°©ë¬¸ì 100ëª… ì¤‘ ëª‡ ëª…ì´ êµ¬ë§¤/ì˜ˆì•½í•˜ì‹œë‚˜ìš”?
ë§Œì•½ 2%ë¼ë©´, AIë¡œ 3%ë§Œ ì˜¬ë ¤ë„ ì›”ë§¤ì¶œì´ 50% ëŠ˜ì–´ë‚©ë‹ˆë‹¤.

íˆ¬ì ëŒ€ë¹„ ìˆ˜ìµ(ROI)ì„ ë¨¼ì € ê³„ì‚°í•´ë³´ì‹œê² ì–´ìš”?"""
        
        elif any(word in user_lower for word in ['íš¨ê³¼', 'ì§„ì§œ', 'ì •ë§']):
            case = CASE_STUDIES.get(context.get('user_type', 'hospital'))
            return f"""ë‹¹ì—°íˆ ì˜ì‹¬ìŠ¤ëŸ¬ìš°ì‹¤ ê²ë‹ˆë‹¤. ê·¼ë° ëŒ€í‘œë‹˜, ì§€ê¸ˆ ì €ì™€ ëŒ€í™”í•˜ì‹œë©´ì„œ ëŠë¼ì…¨ë‚˜ìš”?

ì œê°€ ì‚¬ëŒì²˜ëŸ¼ ëŒ€ë‹µí•œë‹¤ëŠ” ê±¸?

ì‹¤ì œë¡œ <b>{case['title']}</b>ëŠ” ë„ì… í›„ <b>{case['result']}</b> ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

"{case['quote']}"

ì‹¤ì œ ì‚¬ë¡€ë¥¼ ë” ë³´ì‹œê² ì–´ìš”?"""
        
        elif any(word in user_lower for word in ['ì‹œê°„', 'ë°”ì˜', 'ë‚˜ì¤‘']):
            return """ëŒ€í‘œë‹˜, ë”± 2ë¶„ë§Œ íˆ¬ìí•˜ì„¸ìš”.
            
ì§€ê¸ˆ ê²½ìŸì‚¬ë“¤ì€ AIë¡œ ì•¼ê°„/ì£¼ë§ ê³ ê°ê¹Œì§€ ì¡ê³  ìˆìŠµë‹ˆë‹¤.
ëŒ€í‘œë‹˜ì´ 'ë‚˜ì¤‘ì—'ë¥¼ ê³ ë¯¼í•˜ëŠ” ì‚¬ì´, ê³ ê°ì€ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ê°‘ë‹ˆë‹¤.

ë¬´ë£Œ ì„¤ê³„ë„ëŠ” ë°›ì•„ë‘ì‹œê³  ê²€í† í•˜ì…”ë„ ë©ë‹ˆë‹¤. ì†í•´ ë³¼ ê²Œ ì—†ì–ì•„ìš”?"""
        
        else:
            return """ë§ì”€ ê°ì‚¬í•©ë‹ˆë‹¤. ë” ìì„¸íˆ ë“£ê³  ì‹¶ì€ë°ìš”,

ì§€ê¸ˆ ê°€ì¥ ë‹µë‹µí•œ ë¶€ë¶„ì´ ë­”ê°€ìš”?
1ï¸âƒ£ ê´‘ê³ ë¹„ ëŒ€ë¹„ ë§¤ì¶œì´ ì•ˆ ë‚˜ì™€ì„œ?
2ï¸âƒ£ ê³ ê°ì´ ë¬¸ì˜ë§Œ í•˜ê³  êµ¬ë§¤/ì˜ˆì•½ ì•ˆ í•´ì„œ?
3ï¸âƒ£ ì§ì›ë“¤ì´ ì•¼ê·¼í•´ë„ ëŒ€ì‘ì´ ì•ˆ ë¼ì„œ?

í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”."""
    
    def generate_initial_message(self) -> str:
        """
        ì²« ì¸ì‚¬ ë©”ì‹œì§€ ìƒì„± (ê³ ì •)
        
        Returns:
            ì²« ë©”ì‹œì§€
        """
        return """ë°˜ê°‘ìŠµë‹ˆë‹¤. <b>IMD ìˆ˜ì„ ì•„í‚¤í…íŠ¸ AI</b>ì…ë‹ˆë‹¤.

ëŒ€í‘œë‹˜, ì†”ì§íˆ ë§ì”€ë“œë¦¬ì£ .

ì§€ê¸ˆ <b>ë§ˆì¼€íŒ… ë¹„ìš© ëŒ€ë¹„ íš¨ìœ¨(ROAS)</b>, ë§Œì¡±í•˜ì‹œë‚˜ìš”?"""


# ============================================
# í¸ì˜ í•¨ìˆ˜
# ============================================
def get_prompt_engine() -> PromptEngine:
    """PromptEngine ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    if 'prompt_engine' not in st.session_state:
        st.session_state.prompt_engine = PromptEngine()
    return st.session_state.prompt_engine


def generate_ai_response(user_input: str, context: Dict, history: str) -> str:
    """
    ë¹ ë¥¸ AI ì‘ë‹µ ìƒì„± (ì•±ì—ì„œ ë°”ë¡œ í˜¸ì¶œìš©)
    
    Args:
        user_input: ì‚¬ìš©ì ë©”ì‹œì§€
        context: ì»¨í…ìŠ¤íŠ¸
        history: ëŒ€í™” íˆìŠ¤í† ë¦¬
    
    Returns:
        AI ì‘ë‹µ
    """
    engine = get_prompt_engine()
    return engine.generate_response(user_input, context, history)
